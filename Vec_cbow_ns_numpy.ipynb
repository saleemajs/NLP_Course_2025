{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c0eb0f9-44c3-4434-bb56-52ab6d97ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# ---------- Toy corpus ----------\n",
    "sentences = [\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"We are learning natural language processing\",\n",
    "    \"Machine learning powers modern NLP applications\",\n",
    "    \"Natural language processing is a fascinating field\",\n",
    "    \"Deep learning improves NLP performance\",\n",
    "    \"We enjoy exploring text mining techniques\",\n",
    "    \"AI is transforming language understanding\"\n",
    "]\n",
    "tokens = [w.lower() for s in sentences for w in s.split()]\n",
    "vocab_counter = Counter(tokens)\n",
    "vocab = list(vocab_counter.keys())\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856adb3d-e4cf-47be-8f66-98840767ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 29 training pairs: 40\n"
     ]
    }
   ],
   "source": [
    "# ---------- Generate (context_list, target) pairs ----------\n",
    "window = 2\n",
    "pairs = []\n",
    "for s in sentences:\n",
    "    words = s.lower().split()\n",
    "    for i, target in enumerate(words):\n",
    "        start = max(0, i - window)\n",
    "        end   = min(len(words), i + window + 1)\n",
    "        context_indices = [word2idx[words[j]] for j in range(start, end) if j != i]\n",
    "        target_idx = word2idx[target]\n",
    "        pairs.append((context_indices, target_idx))\n",
    "\n",
    "print(\"Vocab size:\", len(vocab), \"training pairs:\", len(pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81adb4f-d9b1-4cf7-8cce-048d523e5ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss ~ 166.354\n",
      "Epoch 2/5, loss ~ 166.350\n",
      "Epoch 3/5, loss ~ 166.348\n",
      "Epoch 4/5, loss ~ 166.339\n",
      "Epoch 5/5, loss ~ 166.339\n"
     ]
    }
   ],
   "source": [
    "# ---------- Negative sampling distribution ----------\n",
    "counts = np.array([vocab_counter[idx2word[i]] for i in range(len(vocab))], dtype=np.float64)\n",
    "probs = counts ** 0.75\n",
    "probs /= probs.sum()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# ---------- Initialize embeddings ----------\n",
    "V = len(vocab)\n",
    "D = 50\n",
    "lr = 0.05\n",
    "K = 5  # negatives per positive\n",
    "\n",
    "W_in  = (np.random.rand(V, D) - 0.5) / D\n",
    "W_out = (np.random.rand(V, D) - 0.5) / D\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(pairs)\n",
    "    loss_epoch = 0.0\n",
    "    for context_indices, target_idx in pairs:\n",
    "        # Average the context word embeddings\n",
    "        v_context = np.mean(W_in[context_indices], axis=0)  # shape (D,)\n",
    "\n",
    "        # Positive sample\n",
    "        score_pos = sigmoid(np.dot(W_out[target_idx], v_context))\n",
    "        loss_epoch += -np.log(score_pos + 1e-10)\n",
    "\n",
    "        # Gradients for positive\n",
    "        grad_u_t = (score_pos - 1.0) * v_context\n",
    "        grad_v_context = (score_pos - 1.0) * W_out[target_idx]\n",
    "        W_out[target_idx] -= lr * grad_u_t\n",
    "\n",
    "        # Negative samples\n",
    "        neg_samples = np.random.choice(V, size=K, p=probs)\n",
    "        for neg in neg_samples:\n",
    "            score_neg = sigmoid(np.dot(W_out[neg], v_context))\n",
    "            loss_epoch += -np.log(1.0 - score_neg + 1e-10)\n",
    "            grad_u_neg = score_neg * v_context\n",
    "            grad_v_context += score_neg * W_out[neg]\n",
    "            W_out[neg] -= lr * grad_u_neg\n",
    "\n",
    "        # Update each context word\n",
    "        for c_idx in context_indices:\n",
    "            W_in[c_idx] -= lr * (grad_v_context / len(context_indices))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, loss ~ {loss_epoch:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e9cb36e-1ccb-45f9-ba16-cfa8a7d13a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest to nlp -> [('powers', 0.3482710520408113), ('understanding', 0.29657599526704337), ('are', 0.28750537555905237), ('improves', 0.2865388272223649), ('techniques', 0.27809960584268634)]\n",
      "\n",
      "Nearest to learning -> [('is', 0.4903240842818587), ('fun', 0.40522353027210717), ('powers', 0.347750950763279), ('exciting', 0.31798843211291145), ('language', 0.31298151894059356)]\n",
      "\n",
      "Nearest to language -> [('learning', 0.31298151894059356), ('natural', 0.2803305050258882), ('we', 0.25962044106488374), ('is', 0.2531892627811632), ('transforming', 0.227953075888446)]\n",
      "\n",
      "Nearest to deep -> [('understanding', 0.24393558308365818), ('learning', 0.1600076413356034), ('and', 0.13360308293215115), ('is', 0.10950010421780249), ('improves', 0.0991060483339744)]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Nearest neighbors ----------\n",
    "def cosine_sim_matrix(mat):\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    normed = mat / (norms + 1e-10)\n",
    "    return np.dot(normed, normed.T)\n",
    "\n",
    "sim = cosine_sim_matrix(W_in)\n",
    "\n",
    "def topk(word, k=5):\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    i = word2idx[word]\n",
    "    scores = sim[i]\n",
    "    top = np.argsort(-scores)\n",
    "    return [(idx2word[t], float(scores[t])) for t in top[1:k+1]]\n",
    "\n",
    "# Examples\n",
    "for w in [\"nlp\", \"learning\", \"language\", \"deep\"]:\n",
    "    print(\"\\nNearest to\", w, \"->\", topk(w, k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01099b16-58c3-440a-aa2e-510fb94b279f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
