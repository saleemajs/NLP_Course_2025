{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4c25bc-e6fe-41bd-9576-f524f0caded4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from math import exp\n",
    "import random\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# ---------- Toy corpus ----------\n",
    "sentences = [\n",
    "    \"NLP is fun and exciting\",\n",
    "    \"We are learning natural language processing\",\n",
    "    \"Machine learning powers modern NLP applications\",\n",
    "    \"Natural language processing is a fascinating field\",\n",
    "    \"Deep learning improves NLP performance\",\n",
    "    \"We enjoy exploring text mining techniques\",\n",
    "    \"AI is transforming language understanding\"\n",
    "]\n",
    "tokens = [w.lower() for s in sentences for w in s.split()]\n",
    "vocab_counter = Counter(tokens)\n",
    "vocab = [w for w, c in vocab_counter.items()]\n",
    "word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx2word = {i:w for w,i in word2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1550ea02-30d6-4c7a-93a5-145d12f3ea75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 29 training pairs: 118\n"
     ]
    }
   ],
   "source": [
    "# ---------- Generate (center, context) training pairs (skip-gram) ----------\n",
    "window = 2\n",
    "pairs = []\n",
    "for s in sentences:\n",
    "    words = s.lower().split()\n",
    "    for i, center in enumerate(words):\n",
    "        center_idx = word2idx[center]\n",
    "        for j in range(max(0, i-window), min(len(words), i+window+1)):\n",
    "            if j == i: \n",
    "                continue\n",
    "            context = words[j]\n",
    "            pairs.append((center_idx, word2idx[context]))\n",
    "\n",
    "print(\"Vocab size:\", len(vocab), \"training pairs:\", len(pairs))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b54a3326-2065-41d2-bf77-9ae03959fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, loss ~ 490.734\n",
      "Epoch 2/5, loss ~ 490.699\n",
      "Epoch 3/5, loss ~ 490.609\n",
      "Epoch 4/5, loss ~ 490.414\n",
      "Epoch 5/5, loss ~ 489.912\n"
     ]
    }
   ],
   "source": [
    "# ---------- Negative sampling distribution (unigram^0.75) ----------\n",
    "counts = np.array([vocab_counter[idx2word[i]] for i in range(len(vocab))], dtype=np.float64)\n",
    "probs = counts ** 0.75\n",
    "probs = probs / probs.sum()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "# ---------- Initialize embeddings ----------\n",
    "V = len(vocab)\n",
    "D = 50         # embedding dimension (small for demo)\n",
    "lr = 0.05\n",
    "K = 5          # negatives per positive\n",
    "\n",
    "W_in = (np.random.rand(V, D) - 0.5) / D   # input embeddings (v)\n",
    "W_out = (np.random.rand(V, D) - 0.5) / D  # output embeddings (u)\n",
    "\n",
    "# ---------- Training loop ----------\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(pairs)\n",
    "    loss_epoch = 0.0\n",
    "    for center_idx, context_idx in pairs:\n",
    "        v_c = W_in[center_idx]          # D\n",
    "        u_o = W_out[context_idx]        # D\n",
    "\n",
    "        # positive score\n",
    "        score_pos = sigmoid(np.dot(u_o, v_c))\n",
    "        loss_epoch += -np.log(score_pos + 1e-10)\n",
    "\n",
    "        # negative samples\n",
    "        neg_samples = np.random.choice(V, size=K, p=probs)\n",
    "        # allow negatives to possibly include the context word; in practice you filter it\n",
    "        # compute gradients\n",
    "        grad_v = (score_pos - 1.0) * u_o    # start with pos grad contribution\n",
    "        grad_uo = (score_pos - 1.0) * v_c\n",
    "        # update output embedding for positive\n",
    "        W_out[context_idx] -= lr * grad_uo\n",
    "\n",
    "        for neg in neg_samples:\n",
    "            u_n = W_out[neg]\n",
    "            score_neg = sigmoid(np.dot(u_n, v_c))\n",
    "            loss_epoch += -np.log(1.0 - score_neg + 1e-10)\n",
    "            grad_un = score_neg * v_c\n",
    "            grad_v += score_neg * u_n\n",
    "            W_out[neg] -= lr * grad_un\n",
    "\n",
    "        # update center embedding after accumulating all contributions\n",
    "        W_in[center_idx] -= lr * grad_v\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, loss ~ {loss_epoch:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece5728d-6441-4a6b-bb33-f1269009b339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nearest to nlp -> [('learning', 0.8811321971428834), ('is', 0.864622500598743), ('powers', 0.843675401786075), ('language', 0.8383270172722006), ('text', 0.7946848200413338)]\n",
      "\n",
      "Nearest to learning -> [('is', 0.9315716235518363), ('nlp', 0.8811321971428834), ('powers', 0.849972921736926), ('language', 0.8485659891260925), ('fun', 0.8311395996525063)]\n",
      "\n",
      "Nearest to language -> [('is', 0.8656354530129817), ('learning', 0.8485659891260925), ('nlp', 0.8383270172722006), ('natural', 0.8232605197340233), ('a', 0.7699022142049373)]\n",
      "\n",
      "Nearest to deep -> [('is', 0.5538959677290589), ('learning', 0.5339860678717774), ('understanding', 0.526072785198946), ('we', 0.48552356157918825), ('language', 0.4761385875987834)]\n"
     ]
    }
   ],
   "source": [
    "# ---------- Utility: nearest neighbors by cosine similarity ----------\n",
    "def cosine_sim_matrix(mat):\n",
    "    norms = np.linalg.norm(mat, axis=1, keepdims=True)\n",
    "    normed = mat / (norms + 1e-10)\n",
    "    return np.dot(normed, normed.T)\n",
    "\n",
    "sim = cosine_sim_matrix(W_in)\n",
    "\n",
    "def topk(word, k=5):\n",
    "    if word not in word2idx:\n",
    "        return []\n",
    "    i = word2idx[word]\n",
    "    scores = sim[i]\n",
    "    top = np.argsort(-scores)\n",
    "    return [(idx2word[t], float(scores[t])) for t in top[1:k+1]]\n",
    "\n",
    "# Examples\n",
    "for w in [\"nlp\", \"learning\", \"language\", \"deep\"]:\n",
    "    print(\"\\nNearest to\", w, \"->\", topk(w, k=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e55e0d-0d9c-4c97-aa0b-0c7aa9ead081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
